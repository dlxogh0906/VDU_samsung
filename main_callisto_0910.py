import os
import json
import subprocess
from pathlib import Path
import pandas as pd
from PIL import Image, ImageEnhance, ImageFilter
import torch
from transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer
from qwen_vl_utils import process_vision_info
from dots_ocr.utils.image_utils import fetch_image
from dots_ocr.utils.doc_utils import load_images_from_pdf
from dots_ocr.utils.format_transformer import get_formula_in_markdown, clean_text, has_latex_markdown
from dots_ocr.utils.output_cleaner import OutputCleaner
from dots_ocr.utils.consts import MIN_PIXELS, MAX_PIXELS
from dots_ocr.utils.layout_utils import draw_layout_on_image  # ì‹œê°í™”ëŠ” ì•ˆ ì”€
import time
import numpy as np
from transformers import Qwen2VLForConditionalGeneration
import re
import pytesseract
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

MODEL_PATH = os.path.join("model", "dots.ocr", "weights", "DotsOCR")  # ë ˆì´ì•„ì›ƒ ê°ì§€ìš©
CALLISTO_MODEL_PATH = os.path.join("model", "Callisto-OCR3-2B-Instruct")  # Callisto OCR ë¡œì»¬ ëª¨ë¸
SUBMISSION_PATH = os.path.join("output", "submission.csv")
TEMP_DIR = "./temp_images"
DEBUG_JSON_DIR = "./debug_json"

USE_VRAM_LIMIT = False   # True â†’ VRAM ì œí•œ ëª¨ë“œ (A6000ì—ì„œ T4 ì‹œë®¬ë ˆì´ì…˜)
                        # False â†’ ì¼ë°˜ ëª¨ë“œ (T4 ê°™ì€ ì‹¤ì œ 16GB í™˜ê²½)

def run_tesseract(image_pil):
    """í…Œì„œë ‰íŠ¸ OCR ì‹¤í–‰ (kor+eng)"""
    padded = Image.new("RGB", (image_pil.width + 20, image_pil.height + 20), "white")
    padded.paste(image_pil, (10, 10))
    text = pytesseract.image_to_string(
        padded, lang="kor+eng", config="--psm 6"
    )
    return text.strip()

def preprocess_for_layout(image, min_size=1000):
    """ë ˆì´ì•„ì›ƒ íƒì§€ ì „ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ì„ ëª…ë„, í•´ìƒë„, ë…¸ì´ì¦ˆ)"""
    try:
        # í•´ìƒë„ ë³´ì • (ë„ˆë¬´ ì‘ìœ¼ë©´ í™•ëŒ€)
        w, h = image.size
        if max(w, h) < min_size:
            scale = min_size / max(w, h)
            image = image.resize((int(w * scale), int(h * scale)), Image.LANCZOS)

        # ì„ ëª…ë„ ê°•í™”
        image = ImageEnhance.Contrast(image).enhance(1.2)
        image = ImageEnhance.Sharpness(image).enhance(1.2)

        # ë…¸ì´ì¦ˆ ì œê±° (í•„í„°)
        image = image.filter(ImageFilter.MedianFilter(size=3))

        return image
    except Exception as e:
        print(f"âš ï¸ ë ˆì´ì•„ì›ƒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return image

def is_number_pair(text: str) -> bool:
    """
    (ìˆ«ì),(ìˆ«ì) íŒ¨í„´ì¸ì§€ í™•ì¸
    - ì²œ ë‹¨ìœ„ ì½¤ë§ˆ(ì˜ˆ: 999,999)ë„ í—ˆìš©
    """
    return bool(re.fullmatch(r"^\(\d{1,3}(?:,\d{3})*\),\(\d{1,3}(?:,\d{3})*\)$", text.strip()))

def has_chinese(text: str, threshold: float = 0.2) -> bool:
    """í•œìê°€ ì¼ì • ë¹„ìœ¨ ì´ìƒ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ True"""
    if not text:
        return False
    total = len(text)
    chinese = sum('\u4e00' <= ch <= '\u9fff' for ch in text)
    return (chinese / total) >= threshold


def limit_vram(max_gb=16, device=0):
    """PyTorch VRAM fraction ì œí•œ (ì˜ˆì•½ í¬í•¨)"""
    total = torch.cuda.get_device_properties(device).total_memory
    fraction = max_gb * (1024**3) / total
    fraction = min(1.0, fraction)  # ğŸš§ fractionì€ 0~1ë§Œ í—ˆìš©
    torch.cuda.set_per_process_memory_fraction(fraction, device)
    print(f"ğŸš§ VRAM ì œí•œ ì„¤ì •: {max_gb}GB (GPU ì „ì²´ {total/1024**3:.1f}GB ì¤‘ {fraction*100:.1f}%)")

def load_models():
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # ë ˆì´ì•„ì›ƒ ê°ì§€ìš© ëª¨ë¸ (ê¸°ì¡´ DotsOCR)
    if device.startswith("cuda"):
        if USE_VRAM_LIMIT:
            # â˜… VRAM ì œí•œ ëª¨ë“œ (A6000ì—ì„œ T4 ì‹œë®¬ë ˆì´ì…˜)
            limit_vram(16, 0)
            layout_model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                device_map="cuda",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                local_files_only=True,
                attn_implementation="sdpa",
                max_memory={0: "8GiB", "cpu": "16GiB"}  # ë©”ëª¨ë¦¬ ë¶„ë°° ì¡°ì •
            )
        else:
            # â˜… ì¼ë°˜ ëª¨ë“œ (T4 ê°™ì€ ì‹¤ì œ 16GB í™˜ê²½)
            layout_model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                device_map="cuda",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                attn_implementation="sdpa",
                local_files_only=True,
            )
    else:
        layout_model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float32,
            trust_remote_code=True,
            local_files_only=True,
            attn_implementation="sdpa",
        )

    layout_processor = AutoProcessor.from_pretrained(MODEL_PATH, use_fast=True)

    # Callisto OCR ëª¨ë¸ ë¡œë”©
    callisto_model = Qwen2VLForConditionalGeneration.from_pretrained(
        CALLISTO_MODEL_PATH,
        torch_dtype=torch.float16,  # float16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        device_map="cuda" if device.startswith("cuda") else "cpu",
        local_files_only=True,
        low_cpu_mem_usage=True,  # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
        attn_implementation="sdpa"  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ attention
    )
    
    # Tokenizerì™€ Processorë¥¼ ë³„ë„ë¡œ ë¡œë“œí•˜ê³  padding_side ì„¤ì •
    callisto_tokenizer = AutoTokenizer.from_pretrained(CALLISTO_MODEL_PATH, local_files_only=True)
    callisto_tokenizer.padding_side = 'left'  # decoder-only ëª¨ë¸ì— ì í•©
    
    callisto_processor = AutoProcessor.from_pretrained(CALLISTO_MODEL_PATH, local_files_only=True)
    callisto_processor.tokenizer.padding_side = 'left'  # processor ë‚´ë¶€ tokenizerë„ ì„¤ì •
    
    return layout_model, layout_processor, callisto_model, callisto_processor, device

# ------------------------------------------------------
# Step 2: ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# ------------------------------------------------------
def load_data():
    data_path = os.path.join("data", "test.csv")
    df = pd.read_csv(data_path)
    return df

# ------------------------------------------------------
# Step 3: ë¬¸ì„œ ì²˜ë¦¬ í´ë˜ìŠ¤ (ì¶”ë¡  ë¡œì§ í¬í•¨)
# ------------------------------------------------------
class CompetitionProcessor:
    def __init__(self):
        self.category_map = {
            "title": "title",
            "section-header": "subtitle",
            "text": "text",
            "picture": "image",
            "table": "table",
            "formula": "equation",
            "list-item": "text",
            "caption": "text",
        }
        self.cleaner = OutputCleaner()

    def convert_to_images(self, input_path, temp_dir, dpi=200):
        ext = Path(input_path).suffix.lower()
        os.makedirs(temp_dir, exist_ok=True)

        if ext == ".pdf":
            return load_images_from_pdf(input_path, dpi=dpi)
        elif ext == ".pptx":
            subprocess.run(
                ["libreoffice", "--headless", "--convert-to", "pdf", "--outdir", temp_dir, input_path],
                check=True,
            )
            pdf_path = os.path.join(temp_dir, Path(input_path).with_suffix(".pdf").name)
            return load_images_from_pdf(pdf_path, dpi=dpi)
        elif ext in [".jpg", ".jpeg", ".png"]:
            image = Image.open(input_path)
            return [image.convert("RGB")]
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")
        
    def create_prompt(self):
        return """Please output the layout information from this PDF image, 
including each layout's bbox and its category. 
The bbox should be in the format [x1, y1, x2, y2]. 
The layout categories for the PDF document include 
['Caption', 'Footnote', 'Formula', 'List-item', 'Page-footer', 'Page-header', 'Picture', 'Section-header', 'Table', 'Text', 'Title']. 
Do not output the corresponding text. 
The layout result should be in JSON format."""

    def remove_bullet_symbols(self, text):
        """ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ë° ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
        if not text:
            return ""
        
        # ì œê±°í•  ê¸°í˜¸ ì§‘í•©
        bullet_symbols = "â–¶â–·â—€â—â–ºâ—„âŠ™â—‡â—†â– â–¡â–²â–³â–¼â–½â˜…â˜†â€»â–²"
        
        # ë¬¸ìì—´ ì‹œì‘ ë¶€ë¶„ì˜ ê¸°í˜¸ + ê³µë°± ì œê±°
        text = re.sub(rf'^[\s]*[{re.escape(bullet_symbols)}]+[\s]*', '', text)
        
        # ì¤„ ì‹œì‘ ë¶€ë¶„ì˜ ê¸°í˜¸ + ê³µë°± ì œê±° (ì—¬ëŸ¬ ì¤„ ì²˜ë¦¬)
        text = re.sub(rf'^[\s]*[{re.escape(bullet_symbols)}]+[\s]*', '', text, flags=re.MULTILINE)
        
        # í…ìŠ¤íŠ¸ ì „ì²´ì—ì„œ ë“±ì¥í•˜ëŠ” ê¸°í˜¸ ëª¨ë‘ ì œê±°
        text = re.sub(rf'[{re.escape(bullet_symbols)}]', '', text)
        
        return text.strip()

    def process_text_by_category(self, text, category):
        if not text:
            return ""
        
        text = clean_text(text)
        
        # âœ… ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ì œê±° ì¶”ê°€
        text = self.remove_bullet_symbols(text)

        # âœ… ì¼ë°˜ í…ìŠ¤íŠ¸ ê³„ì—´(title, subtitle, text)ë§Œ ë³€í™˜ ì ìš©
        if category in ["title", "subtitle", "text"]:
            # \( ... \) â†’ $...$
            text = re.sub(r'\\\((.*?)\\\)', lambda m: f"${m.group(1).strip()}$", text)
        
        return text.strip()
    def validate_and_scale_bbox(self, bbox, current_size, target_size):
        if not isinstance(bbox, list) or len(bbox) != 4:
            return None
        try:
            x1, y1, x2, y2 = [float(x) for x in bbox]
            current_w, current_h = current_size
            target_w, target_h = target_size
            if x1 < 0 or y1 < 0 or x2 > current_w or y2 > current_h:
                return None
            if x2 <= x1 or y2 <= y1:
                return None
            scale_x = target_w / current_w
            scale_y = target_h / current_h
            return [
                max(0, min(target_w - 1, int(round(x1 * scale_x)))),
                max(0, min(target_h - 1, int(round(y1 * scale_y)))),
                max(1, min(target_w, int(round(x2 * scale_x)))),
                max(1, min(target_h, int(round(y2 * scale_y)))),
            ]
        except Exception:
            return None

    def parse_model_output(self, raw_output):
        try:
            if raw_output.strip().startswith("["):
                parsed = json.loads(raw_output)
            else:
                parsed = None
        except:
            parsed = None
        cleaned = self.cleaner.clean_model_output(parsed if parsed is not None else raw_output)
        return cleaned if isinstance(cleaned, list) else []

    def preprocess_image_for_callisto(self, image, target_size=512):
        """Callisto-OCR3-2Bìš© OCR ì „ì²˜ë¦¬ (bbox ë‹¨ìœ„)"""
        try:
            
            
            # 1. íŒ¨ë”© ì¶”ê°€ (í° ë°°ê²½)
            width, height = image.size
            padding = max(20, min(width, height) // 10)
            padded_image = Image.new('RGB', (width + 2*padding, height + 2*padding), color='white')
            padded_image.paste(image, (padding, padding))
            image = padded_image

            # 2. í¬ê¸° ë³´ì • (ë„ˆë¬´ ì‘ì€ bboxëŠ” í™•ëŒ€)
            w, h = image.size
            min_size = 300
            if w < min_size or h < min_size:
                scale = max(min_size / w, min_size / h)
                image = image.resize((int(w * scale), int(h * scale)), Image.LANCZOS)

            # 3. ëŒ€ë¹„/ì„ ëª…ë„ ê°•í™” (Callisto-friendly)
            image = ImageEnhance.Contrast(image).enhance(1.3)
            image = ImageEnhance.Sharpness(image).enhance(1.3)

            # 4. ìµœì¢… í¬ê¸° ì •ê·œí™” (aspect ìœ ì§€ + padding)
            w, h = image.size
            scale = target_size / max(w, h)
            new_w, new_h = int(w * scale), int(h * scale)
            image = image.resize((new_w, new_h), Image.LANCZOS)

            final_image = Image.new("RGB", (target_size, target_size), color="white")
            final_image.paste(image, ((target_size - new_w) // 2, (target_size - new_h) // 2))

            return final_image

        except Exception as e:
            print(f"âš ï¸ Callisto ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image


    def extract_text_batch_with_callisto_ocr(self, image_pil, bbox_list, callisto_model, callisto_processor, device, category_list=None):
        """Callisto OCRì„ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ OCR ìˆ˜í–‰"""
        if not bbox_list:
            return []
        
        # category_listëŠ” í˜„ì¬ ë¯¸ì‚¬ìš© (í–¥í›„ í™•ì¥ìš©)
        _ = category_list
            
        batch_messages = []
        cropped_images = []
        
        # ë°°ì¹˜ë¡œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        for i, bbox in enumerate(bbox_list):
            x1, y1, x2, y2 = bbox
            w, h = image_pil.width, image_pil.height

            # bbox ì ˆëŒ€ê°’ 5í”½ì…€ í™•ì¥
            dx = 5
            dy = 5

            x1e = max(0, x1 - dx)
            y1e = max(0, y1 - dy)
            x2e = min(w, x2 + dx)
            y2e = min(h, y2 + dy)

            # ë„ˆë¬´ ì‘ì€ ì˜ì—­ì€ ìŠ¤í‚µ
            if (x2e - x1e) < 10 or (y2e - y1e) < 10:
                cropped_images.append(None)
                continue

            cropped = image_pil.crop((x1e, y1e, x2e, y2e)).convert("RGB")
            
            # OCR í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            cropped = self.preprocess_image_for_callisto(cropped)
            cropped_images.append(cropped)
            
            # OCR í”„ë¡¬í”„íŠ¸ (Callistoì— ìµœì í™”)
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": cropped},
                        {"type": "text", "text": "Read and transcribe all visible Korean, English text, numbers, and math expressions. Return only the text, no explanations."}, #Extract the text content from this image.
                    ], # Read and transcribe all visible Korean, English text, numbers, and math expressions. Return only the text, no explanations.
                }
            ]
            batch_messages.append(messages)
        
        # ë°°ì¹˜ ì²˜ë¦¬
        results = []
        batch_size = 6  # Callisto ëª¨ë¸ì— ë§ê²Œ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        
        try:
            for i in range(0, len(batch_messages), batch_size):
                batch = batch_messages[i:i+batch_size]
                batch_crops = cropped_images[i:i+batch_size]
                
                # Noneì¸ ì´ë¯¸ì§€ëŠ” ìŠ¤í‚µ
                valid_batch = [(msg, crop) for msg, crop in zip(batch, batch_crops) if crop is not None]
                if not valid_batch:
                    results.extend([""] * len(batch))
                    continue
                
                # ë°°ì¹˜ ì²˜ë¦¬
                texts = []
                images_batch = []
                
                for messages, _ in valid_batch:
                    text = callisto_processor.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    texts.append(text)
                    image_inputs, _ = process_vision_info(messages)
                    images_batch.extend(image_inputs)
                
                if texts and images_batch:
                    inputs = callisto_processor(
                        text=texts,
                        images=images_batch,
                        padding=True,
                        return_tensors="pt",
                    )
                    inputs = inputs.to(device)
                    
                    with torch.no_grad():
                        # Callistoì— ìµœì í™”ëœ ìƒì„± ì„¤ì •
                        generated_ids = callisto_model.generate(
                            **inputs, 
                            max_new_tokens=1024,  # ë” ê¸´ í…ìŠ¤íŠ¸ ìƒì„± í—ˆìš©
                            do_sample=True,
                            temperature=0.01,  # ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ì„± í–¥ìƒ
                            top_p=0.9,
                            num_beams=1,
                            pad_token_id=callisto_processor.tokenizer.pad_token_id,
                            use_cache=True
                        )
                        generated_ids_trimmed = [
                            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                        ]
                        output_texts = callisto_processor.batch_decode(
                            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=True
                        )
                        # ë©”ëª¨ë¦¬ ì •ë¦¬
                        if device.startswith("cuda"):
                            torch.cuda.empty_cache()
                    
                    # ê²°ê³¼ ì •ë¦¬
                    valid_idx = 0
                    for crop in batch_crops:
                        if crop is not None:
                            text = output_texts[valid_idx] if valid_idx < len(output_texts) else ""
                            results.append(clean_text(text) if text else "")
                            valid_idx += 1
                        else:
                            results.append("")
                else:
                    results.extend([""] * len(batch))
                    
        except Exception as e:
            print(f"ë°°ì¹˜ OCR ì˜¤ë¥˜: {str(e)[:120]}")
            results.extend([""] * (len(bbox_list) - len(results)))
        
        return results

    def extract_text_with_callisto_ocr(self, image_pil, bbox, callisto_model, callisto_processor, device, category=None):
        """ë‹¨ì¼ OCR ìˆ˜í–‰ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
        results = self.extract_text_batch_with_callisto_ocr(image_pil, [bbox], callisto_model, callisto_processor, device, [category])
        return results[0] if results else ""
            
    def process_single_image(self, doc_id, image, layout_model, layout_processor, callisto_model, callisto_processor, device, target_size):
        # âœ… ë ˆì´ì•„ì›ƒ íƒì§€ ì „ì²˜ë¦¬ ì¶”ê°€
        image = preprocess_for_layout(image)

        processed_image = fetch_image(image, min_pixels=MIN_PIXELS, max_pixels=MAX_PIXELS)
        messages = [
            {
                "role": "user",
                "content": [{"type": "image", "image": processed_image}, {"type": "text", "text": self.create_prompt()}],
            }
        ]
        text_input = layout_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, _ = process_vision_info(messages)

        inputs = layout_processor(text=[text_input], images=image_inputs, padding=True, return_tensors="pt")
        if device.startswith("cuda"):
            inputs = inputs.to(device, dtype=torch.float16)
        else:
            inputs = inputs.to(device)

        with torch.no_grad():
            output_ids = layout_model.generate(
                **inputs, 
                max_new_tokens=2048,
                do_sample=False,
                num_beams=1,
                pad_token_id=layout_processor.tokenizer.pad_token_id,
                use_cache=True
             )
        if device.startswith("cuda"):
            allocated = torch.cuda.memory_allocated() / (1024**2)
            reserved = torch.cuda.memory_reserved() / (1024**2)
            print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬ [Doc {doc_id}] â†’ allocated: {allocated:.1f} MB, reserved: {reserved:.1f} MB")
        generated_ids = [output_ids[i][len(inputs.input_ids[i]) :] for i in range(len(output_ids))]
        response = layout_processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)[0]
        print(response)
        elements = self.parse_model_output(response)

        submission_rows = []
        ocr_needed_elements = []  # OCR í•„ìš”í•œ ìš”ì†Œë“¤ ì €ì¥
        model_w, model_h = processed_image.size
        order = 0
        
        # 1ë‹¨ê³„: ëª¨ë“  ë ˆì´ì•„ì›ƒ ìš”ì†Œ ì²˜ë¦¬ ë° OCR ëŒ€ìƒ ìˆ˜ì§‘
        for element in elements:
            raw_category = str(element.get("category", "")).strip().lower()
            mapped_category = self.category_map.get(raw_category)
            if not mapped_category:
                continue

            bbox = element.get("bbox", [])
            scaled_bbox = self.validate_and_scale_bbox(bbox, (model_w, model_h), target_size)
            if not scaled_bbox:
                continue

            orig_bbox = self.validate_and_scale_bbox(bbox, (model_w, model_h), image.size)
            # OCR ìˆ˜í–‰ ì—¬ë¶€ ê²°ì •
            if mapped_category in ["title", "subtitle", "text"]:
                ocr_needed_elements.append((len(submission_rows), element, orig_bbox, mapped_category))
                text_content = ""  # ì¼ë‹¨ ë¹ˆ í…ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
            else:
                text_content = ""  # OCR ë¯¸ìˆ˜í–‰

            submission_rows.append(
                {
                    "ID": doc_id,
                    "category_type": mapped_category,
                    "confidence_score": 0.85,
                    "order": order,
                    "text": text_content,
                    "bbox": f"{scaled_bbox [0]}, {scaled_bbox [1]}, {scaled_bbox [2]}, {scaled_bbox [3]}",
                }
            )
            order += 1
        
        # 2ë‹¨ê³„: ë°°ì¹˜ë¡œ OCR ìˆ˜í–‰
        if ocr_needed_elements:
            bbox_list = [elem[2] for elem in ocr_needed_elements]  # orig_bboxë“¤
            category_list = [elem[3] for elem in ocr_needed_elements]  # categoryë“¤
            
            ocr_results = self.extract_text_batch_with_callisto_ocr(
                image_pil=image,
                bbox_list=bbox_list,
                callisto_model=callisto_model,
                callisto_processor=callisto_processor,
                device=device,
                category_list=category_list
            )
            
            # 3ë‹¨ê³„: OCR ê²°ê³¼ë¥¼ submission_rowsì— ì ìš©
# 3ë‹¨ê³„: OCR ê²°ê³¼ë¥¼ submission_rowsì— ì ìš©
            for i, (row_idx, element, orig_bbox, category) in enumerate(ocr_needed_elements):
                if i < len(ocr_results):
                    text_content = self.process_text_by_category(ocr_results[i], category)

                    # âœ… fallback ì¡°ê±´ ê²€ì‚¬
                    if not text_content or is_number_pair(text_content) or has_chinese(text_content, threshold=0.2):
                        x1, y1, x2, y2 = orig_bbox
                        crop = image.crop((x1, y1, x2, y2)).convert("RGB")

                        # 1ì°¨ fallback â†’ Callisto ì¬ì‹œë„ (í°ìƒ‰ íŒ¨ë”© í¬ê²Œ ë„£ìŒ)
                        padded_crop = self.preprocess_image_for_callisto(crop, target_size=512)
                        retry = self.extract_text_with_callisto_ocr(
                            padded_crop, [0, 0, padded_crop.width, padded_crop.height],
                            callisto_model, callisto_processor, device, category
                        )
                        if retry:
                            text_content = retry
                        else:
                            # 2ì°¨ fallback â†’ Tesseract
                            text_tess = run_tesseract(crop)
                            if text_tess:
                                text_content = text_tess

                    submission_rows[row_idx]["text"] = text_content


        # ì‹œê°í™” ì €ì¥ (ì˜µì…˜)
        SAVE_VISUALIZATION = True
        if SAVE_VISUALIZATION and submission_rows:
            try:
                vis_img = image.resize(target_size)
                vis_img = draw_layout_on_image(
                    vis_img,
                    [
                        {"bbox": [int(v) for v in row["bbox"].split(",")],
                        "category": row["category_type"].capitalize()}
                        for row in submission_rows
                    ]
                )
                vis_dir = "./output/vis"
                os.makedirs(vis_dir, exist_ok=True)
                vis_path = os.path.join(vis_dir, f"{doc_id}.png")
                vis_img.save(vis_path)
                print(f"ğŸ–¼ï¸ ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {vis_path}")
            except Exception as e:
                print(f"âš ï¸ ì‹œê°í™” ì €ì¥ ì‹¤íŒ¨ ({doc_id}): {e}")
        return submission_rows

# ------------------------------------------------------
# Step 4: predict (ì¶”ë¡  í•¨ìˆ˜)
# ------------------------------------------------------
def predict(layout_model, layout_processor, callisto_model, callisto_processor, device, df):
    processor_instance = CompetitionProcessor()
    all_submission_rows = []
    csv_dir = os.path.dirname(os.path.join("data", "test.csv"))

    for _, row in df.iterrows():
        doc_id = row["ID"]
        raw_path = row["path"]
        file_path = os.path.normpath(os.path.join(csv_dir, raw_path))
        target_size = (int(row["width"]), int(row["height"]))

        if not os.path.exists(file_path):
            print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}")
            continue

        try:
            images = processor_instance.convert_to_images(file_path, TEMP_DIR)
            for page_idx, img in enumerate(images):
                page_id = f"{doc_id}_p{page_idx+1}" if len(images) > 1 else doc_id
                try:
                    page_results = processor_instance.process_single_image(
                        doc_id=page_id,
                        image=img,
                        layout_model=layout_model,
                        layout_processor=layout_processor,
                        callisto_model=callisto_model,
                        callisto_processor=callisto_processor,
                        device=device,
                        target_size=target_size,
                    )
                    all_submission_rows.extend(page_results)
                except Exception as e:
                    print(f"âŒ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {page_id} â†’ {e}")
                finally:
                    # ë©”ëª¨ë¦¬ ì •ë¦¬ ê°•í™”
                    if device.startswith("cuda"):
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                    
                    # ì´ë¯¸ì§€ ê°ì²´ ë©”ëª¨ë¦¬ í•´ì œ
                    del img
                    if 'page_results' in locals():
                        del page_results
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path} â†’ {e}")
            continue

    return all_submission_rows

# ------------------------------------------------------
# Step 5: save_results (ê²°ê³¼ ì €ì¥)
# ------------------------------------------------------
def save_results(predictions):
    os.makedirs("output", exist_ok=True)
    submission_df = pd.DataFrame(predictions, columns=["ID", "category_type", "confidence_score", "order", "text", "bbox"])
    submission_df.to_csv(SUBMISSION_PATH, index=False, encoding="UTF-8-sig")
    print(f"ì œì¶œíŒŒì¼ ì €ì¥ ì™„ë£Œ: {SUBMISSION_PATH}, ì´ í•­ëª© ìˆ˜ {len(submission_df)}")

# ------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰
# ------------------------------------------------------
if __name__ == "__main__":
    
    start_time = time.time()   # ì‹œì‘ ì‹œê° ê¸°ë¡
    
    layout_model, layout_processor, callisto_model, callisto_processor, device = load_models()
    df = load_data()
    predictions = predict(layout_model, layout_processor, callisto_model, callisto_processor, device, df)
    if predictions:
        save_results(predictions)
    else:
        print("ê²°ê³¼ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    end_time = time.time()     # ë ì‹œê° ê¸°ë¡
    elapsed = end_time - start_time
    print(f"â±ï¸ ì´ ì¶”ë¡  ì‹œê°„: {elapsed:.2f}ì´ˆ ({elapsed/60:.2f}ë¶„)")